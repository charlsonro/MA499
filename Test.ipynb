{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "import scipy.misc\n",
    "import scipy\n",
    "import time\n",
    "import skimage.io\n",
    "from skimage.io import imread\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras import optimizers, metrics, regularizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, SeparableConv2D, core\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Cropping2D, concatenate, Input\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.activations import softmax\n",
    "import matplotlib\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_weight_one = 1\n",
    "class_weight_two = 1\n",
    "batch_size = 25\n",
    "epochs_2_train = 20\n",
    "validPercent = 0.1\n",
    "image_folder = 'C:\\\\Users\\\\x01429\\\\Desktop\\\\32x_Sample\\\\' #Source of images\n",
    "label_folder = 'C:\\\\Users\\\\x01429\\\\Desktop\\\\32x_Otsu\\\\' #Otsu images\n",
    "prediction_folder = 'C:\\\\Users\\\\x01429\\\\Desktop\\\\32x_Predictions\\\\' #Folder for predictions\n",
    "\n",
    "def get_sample(sample,image_folder,label_folder):\n",
    "\timg_int = np.array(imread(image_folder + sample), dtype = 'float32')/255\n",
    "\tlbl_int = np.array(imread(label_folder + sample), dtype = 'float32')/2**16\n",
    "\tlbl_int[:,:,1] = 1-lbl_int[:,:,0]\n",
    "\tlbl_int = lbl_int[:,:,0:2]\n",
    "\treturn img_int,lbl_int\n",
    "\n",
    "def get_sample_test(sample,test_folder):\n",
    "\timg_int = np.array(imread(test_folder + sample), dtype = 'float32')/255\n",
    "\treturn img_int\n",
    "\n",
    "def data_generator(file_name_list,batch_size): #randomly sampled instances from file name list with batch size\n",
    "\twhile True:\n",
    "\t\tbatch_filenames = np.random.choice(file_name_list,batch_size)\n",
    "\t\tbatch_input = []\n",
    "\t\tbatch_output = []\n",
    "\t\tfor i_filename in batch_filenames:\n",
    "\t\t\tAi_img, Ai_mask = get_sample(i_filename,image_folder,label_folder)\n",
    "\t\t\tbatch_input += [Ai_img]\n",
    "\t\t\tbatch_output += [Ai_mask]\n",
    "\t\tbatch_img = np.expand_dims(np.array(batch_input,dtype='float32'),axis=-1)\n",
    "\t\tbatch_mask = np.array(batch_output,dtype='float32')\n",
    "\t\tyield (batch_img,batch_mask)\n",
    "\n",
    "def Weighted_Binary_CrossEntropy(y_true_n,y_pred_n):\n",
    "\tb_ce = K.binary_crossentropy(y_true_n,y_pred_n)\n",
    "\ty_true = K.cast(K.expand_dims(K.argmax(y_true_n,axis=-1),axis=-1),dtype='float32')\n",
    "\ty_pred = K.cast(K.expand_dims(K.argmax(y_pred_n,axis=-1),axis=-1),dtype='float32')\n",
    "\t#Pixel Disparity\n",
    "\tone_weight = class_weight_one\n",
    "\tzero_weight = class_weight_two\n",
    "\tweight_vector = y_true * one_weight + (1.-y_true)*zero_weight\n",
    "\tweighted_b_ce = weight_vector*b_ce\n",
    "\treturn K.mean(weighted_b_ce)\n",
    "\n",
    "\n",
    "sampleList = os.listdir(image_folder)\n",
    "\n",
    "indexes_valid = random.sample(range(0,len(sampleList)),int(validPercent*float(len(sampleList))))\n",
    "indexes_train = [x for x in range(0,len(sampleList)) if x not in indexes_valid]\n",
    "training_list = [x for ind,x in enumerate(sampleList) if ind in indexes_train]\n",
    "valid_list = [x for ind,x in enumerate(sampleList) if ind in indexes_valid]\n",
    "#(batch1,batch2) = data_generator(training_list,batch_size)\n",
    "#print(batch1.shape)\n",
    "#print(batch2.shape)\n",
    "\n",
    "'''\n",
    "\n",
    "test1,test2 = get_sample(training_list[0])\n",
    "print(np.max(test2))\n",
    "print(np.min(test2))\n",
    "matplotlib.image.imsave('test1.png',test1)\n",
    "matplotlib.image.imsave('test2.png',test2)\n",
    "\n",
    "'''\n",
    "num_train_calls = int(float(len(training_list)+1)/float(batch_size))\n",
    "num_valid_calls = int(float(len(valid_list)+1)/float(batch_size))\n",
    "\n",
    "#### Network is built ####\n",
    "network = Sequential()\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),input_shape = (32,32,1),\n",
    "padding = 'same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Dropout(0.25))\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Dropout(0.25))\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Dropout(0.25))\n",
    "network.add(Conv2D(2,(1,1),activation = 'softmax',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "sgd = optimizers.SGD(lr = 0.0001, decay = 1e-8, momentum = 0.9, nesterov = False)\n",
    "network.compile(loss = Weighted_Binary_CrossEntropy, optimizer = sgd, metrics = ['accuracy'])\n",
    "network.summary()\n",
    "#network.load_weights('weights.h5')\n",
    "network_training = network.fit_generator(data_generator(training_list,batch_size),\n",
    "steps_per_epoch = 50, epochs = epochs_2_train, verbose = 1,validation_data = \n",
    "data_generator(valid_list,batch_size),validation_steps = num_valid_calls)\n",
    "#network.save_weights('weights.h5')\n",
    "TP = 0 #True Positives\n",
    "TN = 0 #True Negatives\n",
    "FP = 0 #False Positives\n",
    "FN = 0 #False Negatives\n",
    "\n",
    "for filename in valid_list: #Use for new lists\n",
    "\timg,lbl = get_sample(filename,image_folder,label_folder)\n",
    "\tlbl = np.argmax(lbl,axis=-1)\n",
    "\timg = np.expand_dims(img,axis=0)\n",
    "\timg = np.expand_dims(img,axis=-1)\n",
    "\timg_pred = network.predict(img)\n",
    "\tpred = np.argmax(img_pred[0,:,:,:],axis=-1)\n",
    "\tTP += np.sum(pred*lbl,dtype='float32')\n",
    "\tTN += np.sum((1-pred)*(1-lbl),dtype='float32')\n",
    "\tFP += np.sum((1-pred)*lbl,dtype='float32')\n",
    "\tFN += np.sum(pred*(1-lbl),dtype='float32')\n",
    "\n",
    "ACC = (TP + TN)/(TP + TN + FP + FN) #Accuracy\n",
    "Recall = (TP)/(TP + FN)\n",
    "Precision = (TP)/(TP + FP)\n",
    "MCC = ((TP * TN) - (FP * FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "\n",
    "np.savetxt('ValidationMetrics2.txt',(ACC,Recall,Precision,MCC))\n",
    "np.savetxt('ValidationLoss2.txt',network_training.history['val_loss'])\n",
    "np.savetxt('ValidationAcc2.txt',network_training.history['val_acc'])\n",
    "\n",
    "'''\n",
    "plt.plot([1,2])\n",
    "plt.ylabel(network_training.history['val_acc'])\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "test_list = valid_list\n",
    "\n",
    "### Generate predictions from trained model ###\n",
    "for filename in test_list:\n",
    "\timg,lbl = get_sample(filename,image_folder,label_folder)\n",
    "\tlbl = np.argmax(lbl,axis=-1)\n",
    "\timg = np.expand_dims(img,axis=0)\n",
    "\timg = np.expand_dims(img,axis=-1)\n",
    "\timg_pred = network.predict(img)\n",
    "\tpred = np.argmax(img_pred[0,:,:,:],axis=-1)\n",
    "\tmatplotlib.image.imsave(prediction_folder+filename,pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
