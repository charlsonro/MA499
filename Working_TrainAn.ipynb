{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 1024, 1024, 10)    100       \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 1024, 1024, 10)    40        \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 1024, 1024, 10)    910       \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 1024, 1024, 10)    40        \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024, 1024, 10)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 1024, 1024, 10)    910       \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 1024, 1024, 10)    40        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1024, 1024, 10)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 1024, 1024, 10)    910       \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1024, 1024, 10)    40        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024, 1024, 10)    0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1024, 1024, 2)     22        \n",
      "=================================================================\n",
      "Total params: 3,012\n",
      "Trainable params: 2,932\n",
      "Non-trainable params: 80\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/skimage/external/tifffile/tifffile.py:2616: RuntimeWarning: py_decodelzw encountered unexpected end of stream\n",
      "  strip = decompress(strip)\n",
      "/opt/conda/lib/python3.7/site-packages/skimage/external/tifffile/tifffile.py:2551: UserWarning: unpack: buffer size must be a multiple of element size\n",
      "  warnings.warn(\"unpack: %s\" % e)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 19s 384ms/step - loss: 1.0183 - accuracy: 0.7169 - val_loss: 2.9074 - val_accuracy: 0.3301\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 18s 357ms/step - loss: 0.7790 - accuracy: 0.8655 - val_loss: 1.5050 - val_accuracy: 0.0544\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 18s 358ms/step - loss: 0.7000 - accuracy: 0.8970 - val_loss: 1.0215 - val_accuracy: 0.6483\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 18s 353ms/step - loss: 0.6547 - accuracy: 0.9171 - val_loss: 0.7972 - val_accuracy: 0.6517\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 18s 357ms/step - loss: 0.6431 - accuracy: 0.9200 - val_loss: 0.8390 - val_accuracy: 0.7419\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 0.6141 - accuracy: 0.9339 - val_loss: 0.7627 - val_accuracy: 0.9143\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 18s 353ms/step - loss: 0.6237 - accuracy: 0.9267 - val_loss: 0.5678 - val_accuracy: 0.9604\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 18s 356ms/step - loss: 0.5710 - accuracy: 0.9473 - val_loss: 0.4759 - val_accuracy: 0.9673\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 0.5742 - accuracy: 0.9492 - val_loss: 0.4855 - val_accuracy: 0.9702\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 18s 355ms/step - loss: 0.5638 - accuracy: 0.9464 - val_loss: 0.4674 - val_accuracy: 0.9715\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'val_acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b07983f59d6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ValidationMetrics2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mACC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRecall\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPrecision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMCC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ValidationLoss2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavetxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ValidationAcc2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnetwork_training\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m '''\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_acc'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "import scipy.misc\n",
    "import scipy\n",
    "import time\n",
    "import skimage.io\n",
    "from skimage.io import imread\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras import optimizers, metrics, regularizers\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, SeparableConv2D, core\n",
    "from keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Conv2DTranspose, Cropping2D, concatenate, Input\n",
    "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from keras.activations import softmax\n",
    "import matplotlib\n",
    "import sklearn.metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "class_weight_one = 1\n",
    "class_weight_two = 1\n",
    "batch_size = 2\n",
    "epochs_2_train = 10\n",
    "validPercent = 0.1\n",
    "image_folder = '/data/Ro_ImageData/High SN ratio ceramic images/' #Source of images\n",
    "label_folder = '/data/Ro_ImageData/32xOTSU/' #Otsu images\n",
    "prediction_folder = 'work/MA499/Predictions_test/' #Folder for predictions\n",
    "\n",
    "def get_sample(sample,image_folder,label_folder):\n",
    "    img_int = np.array(imread(image_folder + sample), dtype = 'float32')/255\n",
    "    lbl_int = np.array(imread(label_folder + sample), dtype = 'float32')/2**16\n",
    "    lbl_int[:,:,1] = 1-lbl_int[:,:,0]\n",
    "    lbl_int = lbl_int[:,:,0:2]\n",
    "    return img_int,lbl_int\n",
    "\n",
    "def get_sample_test(sample,test_folder):\n",
    "    img_int = np.array(imread(test_folder + sample), dtype = 'float32')/255\n",
    "    return img_int\n",
    "\n",
    "def data_generator(file_name_list,batch_size): #randomly sampled instances from file name list with batch size\n",
    "    while True:\n",
    "        batch_filenames = np.random.choice(file_name_list,batch_size)\n",
    "        batch_input = []\n",
    "        batch_output = []\n",
    "        for i_filename in batch_filenames:\n",
    "            Ai_img, Ai_mask = get_sample(i_filename,image_folder,label_folder)\n",
    "            batch_input += [Ai_img]\n",
    "            batch_output += [Ai_mask]\n",
    "            #print(i_filename)\n",
    "        batch_img = np.expand_dims(np.array(batch_input,dtype='float32'),axis=-1)\n",
    "        batch_mask = np.array(batch_output,dtype='float32')\n",
    "        yield (batch_img,batch_mask)\n",
    "\n",
    "def Weighted_Binary_CrossEntropy(y_true_n,y_pred_n):\n",
    "    b_ce = K.binary_crossentropy(y_true_n,y_pred_n)\n",
    "    y_true = K.cast(K.expand_dims(K.argmax(y_true_n,axis=-1),axis=-1),dtype='float32')\n",
    "    y_pred = K.cast(K.expand_dims(K.argmax(y_pred_n,axis=-1),axis=-1),dtype='float32')\n",
    "    #Pixel Disparity\n",
    "    one_weight = class_weight_one\n",
    "    zero_weight = class_weight_two\n",
    "    weight_vector = y_true * one_weight + (1.-y_true)*zero_weight\n",
    "    weighted_b_ce = weight_vector*b_ce\n",
    "    return K.mean(weighted_b_ce)\n",
    "\n",
    "\n",
    "sampleList = os.listdir(image_folder)\n",
    "\n",
    "indexes_valid = random.sample(range(0,len(sampleList)),int(validPercent*float(len(sampleList))))\n",
    "indexes_train = [x for x in range(0,len(sampleList)) if x not in indexes_valid]\n",
    "training_list = [x for ind,x in enumerate(sampleList) if ind in indexes_train]\n",
    "valid_list = [x for ind,x in enumerate(sampleList) if ind in indexes_valid]\n",
    "#(batch1,batch2) = data_generator(training_list,batch_size)\n",
    "#print(batch1.shape)\n",
    "#print(batch2.shape)\n",
    "\n",
    "'''\n",
    "\n",
    "test1,test2 = get_sample(training_list[0])\n",
    "print(np.max(test2))\n",
    "print(np.min(test2))\n",
    "matplotlib.image.imsave('test1.png',test1)\n",
    "matplotlib.image.imsave('test2.png',test2)\n",
    "\n",
    "'''\n",
    "num_train_calls = int(float(len(training_list)+1)/float(batch_size))\n",
    "num_valid_calls = int(float(len(valid_list)+1)/float(batch_size))\n",
    "\n",
    "#### Network is built ####\n",
    "network = Sequential()\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),input_shape = (1024,1024,1),\n",
    "padding = 'same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Dropout(0.25))\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Dropout(0.25))\n",
    "network.add(Conv2D(10,(3,3),activation = 'relu',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "network.add(keras.layers.BatchNormalization())\n",
    "network.add(Dropout(0.25))\n",
    "network.add(Conv2D(2,(1,1),activation = 'softmax',kernel_regularizer = regularizers.l2(.01),padding='same'))\n",
    "sgd = optimizers.SGD(lr = 0.0001, decay = 1e-8, momentum = 0.9, nesterov = False)\n",
    "network.compile(loss = Weighted_Binary_CrossEntropy, optimizer = sgd, metrics = ['accuracy'])\n",
    "network.summary()\n",
    "#network.load_weights('weights.h5')\n",
    "network_training = network.fit_generator(data_generator(training_list,batch_size),\n",
    "steps_per_epoch = 50, epochs = epochs_2_train, verbose = 1,validation_data = \n",
    "data_generator(valid_list,batch_size),validation_steps = num_valid_calls)\n",
    "#network.save_weights('weights.h5')\n",
    "TP = 0 #True Positives\n",
    "TN = 0 #True Negatives\n",
    "FP = 0 #False Positives\n",
    "FN = 0 #False Negatives\n",
    "\n",
    "for filename in valid_list: #Use for new lists\n",
    "    img,lbl = get_sample(filename,image_folder,label_folder)\n",
    "    lbl = np.argmax(lbl,axis=-1)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = np.expand_dims(img,axis=-1)\n",
    "    img_pred = network.predict(img)\n",
    "    pred = np.argmax(img_pred[0,:,:,:],axis=-1)\n",
    "    TP += np.sum(pred*lbl,dtype='float32')\n",
    "    TN += np.sum((1-pred)*(1-lbl),dtype='float32')\n",
    "    FP += np.sum((1-pred)*lbl,dtype='float32')\n",
    "    FN += np.sum(pred*(1-lbl),dtype='float32')\n",
    "\n",
    "ACC = (TP + TN)/(TP + TN + FP + FN) #Accuracy\n",
    "Recall = (TP)/(TP + FN)\n",
    "Precision = (TP)/(TP + FP)\n",
    "MCC = ((TP * TN) - (FP * FN))/np.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))\n",
    "\n",
    "np.savetxt('ValidationMetrics2.txt',(ACC,Recall,Precision,MCC))\n",
    "np.savetxt('ValidationLoss2.txt',network_training.history['val_loss'])\n",
    "np.savetxt('ValidationAcc2.txt',network_training.history['val_accuracy'])\n",
    "\n",
    "'''\n",
    "plt.plot([1,2])\n",
    "plt.ylabel(network_training.history['val_acc'])\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "test_list = valid_list\n",
    "\n",
    "### Generate predictions from trained model ###\n",
    "'''\n",
    "for filename in test_list:\n",
    "    img,lbl = get_sample(filename,image_folder,label_folder)\n",
    "    lbl = np.argmax(lbl,axis=-1)\n",
    "    img = np.expand_dims(img,axis=0)\n",
    "    img = np.expand_dims(img,axis=-1)\n",
    "    img_pred = network.predict(img)\n",
    "    pred = np.argmax(img_pred[0,:,:,:],axis=-1)\n",
    "    matplotlib.image.imsave(prediction_folder+filename,pred)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
